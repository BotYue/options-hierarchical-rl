{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from four_rooms import FourRoomsEnvironment\n",
    "from option import Option\n",
    "\n",
    "\"\"\" Simple agent planning using Q-learning \"\"\"\n",
    "class SMDPQLearningAgent():\n",
    "    def __init__(self, gamma=0.9):\n",
    "        self.options = \\\n",
    "         [Option(\"left\"), Option(\"up\"), Option(\"right\"), Option(\"down\"),\n",
    "          Option(\"topleft->topright\"), Option(\"topleft->botleft\"),\n",
    "          Option(\"topright->topleft\"), Option(\"topright->botright\"),\n",
    "          Option(\"botleft->topleft\"), Option(\"botleft->botright\"),\n",
    "          Option(\"botright->botleft\"), Option(\"botright->topright\")]\n",
    "        \n",
    "        self.gamma = gamma # Discount factor, 0.9 by default as in paper\n",
    "        self.current_option = None\n",
    "        self.starting_state = None # Starting state of current option\n",
    "        self.k = 0   # Number of time steps elapsed in current option\n",
    "        self.cumulative_reward = 0  # Total reward for current option\n",
    "            \n",
    "        # Initialize option value table, and occurrence counts table\n",
    "        n_states = 13 * 13\n",
    "        n_options = len(self.options)\n",
    "        self.Q = np.zeros((n_states, n_options)) \n",
    "        self.N = np.zeros((n_states, n_options))\n",
    "    \n",
    "    def epsilonGreedyPolicy(self, state, epsilon=0.1):\n",
    "        # If we are not currently following an option\n",
    "        if self.current_option is None:\n",
    "            # Pick a new option and record starting state\n",
    "            self._pickNewOptionEpsilonGreedily(state, epsilon)\n",
    "        \n",
    "        # Select action according to policy of current option \n",
    "        action, _ = self.current_option.pickAction(state)\n",
    "        return action\n",
    "        \n",
    "    # Remark : state argument is unused, we update only for the starting\n",
    "    # state of the finishing option (which is recorded in the agent)\n",
    "    def recordTransition(self, state, reward, next_state):\n",
    "        # Add reward discounted by current discounting factor\n",
    "        self.cumulative_reward += (self.gamma ** self.k) * reward\n",
    "        self.k += 1 # Increment k after\n",
    "        \n",
    "        # If current option terminates at next state\n",
    "        if self.current_option.beta[next_state] == 1:\n",
    "            # Update Q table\n",
    "            self._updateQValue(next_state)\n",
    "            # Reset current option to None\n",
    "            self._resetCurrentOption()\n",
    "        \n",
    "    def _updateQValue(self, next_state):\n",
    "        s1 = self._sIdx(self.starting_state)\n",
    "        o = self._oIdx(self.current_option)\n",
    "        s2 = self._sIdx(next_state)\n",
    "        \n",
    "        self.N[s1, o] += 1\n",
    "        alpha = (1. / self.N[s1, o])\n",
    "        \n",
    "        target = self.cumulative_reward + \\\n",
    "            (self.gamma ** self.k) * np.max(self.Q[s2])\n",
    "        self.Q[s1, o] += alpha * (target - self.Q[s1, o])\n",
    "        \n",
    "    # Pick new option according to model and state value function greedily\n",
    "    def _pickNewOptionEpsilonGreedily(self, state, epsilon):\n",
    "        # Iterate over options, keeping track of all available options\n",
    "        # and the index of best option seen so far\n",
    "        available_options = []\n",
    "        best_option_index = 0   \n",
    "        s = self._sIdx(state)\n",
    "        for i in xrange(len(self.options)):\n",
    "            if self.options[i].I[state] == 1:\n",
    "                available_options.append(self.options[i])\n",
    "                if self.Q[s, i] > self.Q[s, best_option_index]:\n",
    "                    best_option_index = i\n",
    "        \n",
    "        # Pick greedy option with probability (1 - epsilon)\n",
    "        if random.uniform(0, 1) > epsilon:\n",
    "            self.current_option = self.options[best_option_index]\n",
    "        \n",
    "        # Pick random action with probability epsilon\n",
    "        else:\n",
    "            self.current_option = random.choice(available_options)\n",
    "            \n",
    "        # Set starting state of option\n",
    "        self.starting_state = state\n",
    "        \n",
    "    def _sIdx(self, state):\n",
    "        return state[0] * 13 + state[1]\n",
    "    \n",
    "    def _oIdx(self, option):\n",
    "        return self.options.index(option)\n",
    "    \n",
    "    def _resetCurrentOption(self):\n",
    "        self.k = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.current_option = None\n",
    "        self.starting_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FourRoomsEnvironment()\n",
    "agent = SMDPQLearningAgent()\n",
    "\n",
    "def run_episode(verbose=False):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = agent.epsilonGreedyPolicy(state)\n",
    "        if verbose:\n",
    "            print(\"State = {}, Option = {}, Action = {}\".format(\n",
    "                state, agent.current_option, action))\n",
    "        next_state, reward, done = env.step(action)\n",
    "        agent.recordTransition(state, reward, next_state)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "for i in xrange(10000):\n",
    "    run_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State = (1, 1), Option = topleft->topright, Action = right\n",
      "State = (2, 1), Option = topleft->topright, Action = right\n",
      "State = (3, 1), Option = topleft->topright, Action = right\n",
      "State = (3, 1), Option = topleft->topright, Action = right\n",
      "State = (2, 1), Option = topleft->topright, Action = right\n",
      "State = (2, 2), Option = topleft->topright, Action = right\n",
      "State = (2, 3), Option = topleft->topright, Action = right\n",
      "State = (2, 2), Option = topleft->topright, Action = right\n",
      "State = (2, 3), Option = topleft->topright, Action = right\n",
      "State = (2, 4), Option = topleft->topright, Action = right\n",
      "State = (2, 5), Option = topleft->topright, Action = down\n",
      "State = (3, 5), Option = topleft->topright, Action = right\n",
      "State = (3, 6), Option = topright->botright, Action = right\n",
      "State = (3, 6), Option = topright->botright, Action = right\n",
      "State = (3, 5), Option = right, Action = right\n",
      "State = (3, 6), Option = topright->botright, Action = right\n",
      "State = (3, 6), Option = topright->botright, Action = right\n",
      "State = (3, 6), Option = topright->botright, Action = right\n",
      "State = (3, 7), Option = topright->botright, Action = down\n",
      "State = (4, 7), Option = topright->botright, Action = down\n",
      "State = (5, 7), Option = topright->botright, Action = down\n",
      "State = (6, 7), Option = topright->botright, Action = right\n",
      "State = (6, 8), Option = topright->botright, Action = right\n",
      "State = (5, 8), Option = topright->botright, Action = down\n",
      "State = (5, 9), Option = topright->botright, Action = down\n",
      "State = (5, 8), Option = topright->botright, Action = down\n",
      "State = (6, 8), Option = topright->botright, Action = right\n",
      "State = (6, 9), Option = topright->botright, Action = down\n",
      "\n",
      "[ 0.01446359  0.06028045  0.05500199  0.03030016  0.08992355  0.01754757\n",
      "  0.          0.          0.          0.          0.          0.        ]\n",
      "[ 0.14098532  0.26562387  0.29846347  0.26577927  0.          0.10971138\n",
      "  0.          0.30538415  0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "run_episode(verbose=True)\n",
    "print\n",
    "print(agent.Q[agent._sIdx((1,1))])\n",
    "print(agent.Q[agent._sIdx((3,6))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
